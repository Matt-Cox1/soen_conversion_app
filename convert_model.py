#!/usr/bin/env python3
# FILEPATH: src/soen_sim_v2/utils/physical_mappings/convert_model.py
# do we even need this file?

import os
import sys
import uuid
import json
import time
import torch
import tempfile
import argparse
import numpy as np
from pathlib import Path
import warnings
# Set matplotlib backend to Agg to avoid GUI issues
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
from typing import Dict, Any, Optional, List, Tuple
import shutil

# Add parent directory to path to ensure module can be imported
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

from model_converter import convert_model_to_physical
from model_converter import extract_param_df
from soen_conversion_utils import PhysicalConverter

class ModelConverter:
    """
    A command-line and web interface for converting SOEN model parameters.
    Handles model loading, parameter conversion, and results export.
    """
    
    def __init__(self, temp_dir: Optional[str] = None):
        """Initialize the model converter with optional temp directory."""
        self.temp_dir = temp_dir or os.path.join(os.path.dirname(os.path.abspath(__file__)), "temp_downloads")
        os.makedirs(self.temp_dir, exist_ok=True)
        
        # Cleanup old temporary files
        self._cleanup_old_temp_files()
        
    def _cleanup_old_temp_files(self, max_age_hours: int = 24):
        """Clean up temporary files older than specified age."""
        try:
            current_time = time.time()
            for item in os.listdir(self.temp_dir):
                item_path = os.path.join(self.temp_dir, item)
                if os.path.isfile(item_path):
                    file_mtime = os.path.getmtime(item_path)
                    file_age_hours = (current_time - file_mtime) / 3600
                    if file_age_hours > max_age_hours:
                        os.remove(item_path)
        except Exception as e:
            print(f"Warning: Could not clean up temp files: {e}")
    
    def load_model(self, model_path: str) -> Dict[str, Any]:
        """
        Load a SOEN model from a .pth file.
        
        Args:
            model_path: Path to the .pth file
            
        Returns:
            Dictionary containing the loaded model data
        """
        try:
            # First try with weights_only=False but explicitly disable security
            # This is safe for our internal model files
            try:
                # For PyTorch â‰¥ 2.0: use safe_load context manager if available
                if hasattr(torch.serialization, 'safe_load'):
                    # This is the modern way (PyTorch 2.1+)
                    with torch.serialization.safe_load(trusted_modules=["soen_sim_v2"]):
                        return torch.load(model_path, map_location=torch.device('cpu'))
                else:
                    # Fallback for older PyTorch versions
                    return torch.load(model_path, map_location=torch.device('cpu'), weights_only=False)
            except Exception as first_error:
                print(f"First load attempt failed: {str(first_error)}")
                
                # Try a second approach with weights_only=True to extract just the parameters
                try:
                    print("Attempting to load model with weights_only=True")
                    model_weights = torch.load(model_path, map_location=torch.device('cpu'), weights_only=True)
                    # Return in a consistent format with model_state_dict as the key
                    return {'model_state_dict': model_weights}
                except Exception as second_error:
                    print(f"Second load attempt failed: {str(second_error)}")
                    
                    # As a last resort for PyTorch 2.1+, try with pickle_module=None which disables pickle checks
                    try:
                        warnings.warn("Using unsafe loading method as last resort. Only use with trusted model files.")
                        result = torch.load(
                            model_path, 
                            map_location=torch.device('cpu'),
                            weights_only=False,
                            pickle_module=None  # Disable pickle safety checks
                        )
                        return result
                    except Exception as third_error:
                        print(f"Third load attempt failed: {str(third_error)}")
                        raise ValueError(f"All load attempts failed, cannot process model file")
        except Exception as e:
            raise ValueError(f"Failed to load model from {model_path}: {e}")
    
    def convert_model(self, 
                     model_path: str, 
                     I_c: float = 100e-6, 
                     gamma_c: float = 1.5e-9, 
                     beta_c: float = 0.3,
                     create_plots: bool = True,
                     output_dir: Optional[str] = None,
                     model_data: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        Convert a SOEN model to physical parameters.
        
        Args:
            model_path: Path to the .pth model file
            I_c: Critical current in Amperes
            gamma_c: Capacitance proportionality in F/A
            beta_c: Stewart-McCumber parameter
            create_plots: Whether to create parameter distribution plots
            output_dir: Directory to save results (if None, uses a temp directory)
            model_data: Optional pre-loaded model data (if provided, model_path is ignored)
            
        Returns:
            Dictionary containing conversion results and metadata
        """
        # Create a unique ID for this model conversion
        model_id = str(uuid.uuid4())
        
        # Determine output directory
        if output_dir is None:
            output_dir = os.path.join(self.temp_dir, model_id)
        
        os.makedirs(output_dir, exist_ok=True)
        
        # Load the model if not provided
        if model_data is None:
            try:
                model_data = self.load_model(model_path)
            except Exception as e:
                print(f"Error loading model: {e}")
                raise ValueError(f"Failed to load model: {e}")
        
        # Extract metadata
        metadata = {
            'model_id': model_id,
            'dt': model_data.get('dt'),
            'epoch': model_data.get('epoch'),
            'k': model_data.get('k'),
        }
        
        # Ensure we have metadata fields, even if empty
        # Note: We don't need to check for model_state_dict like before since load_model already handles this
        if 'dt' not in model_data:
            model_data['dt'] = None
        if 'epoch' not in model_data:
            model_data['epoch'] = None
        if 'k' not in model_data:
            model_data['k'] = None
            
        # Convert model to physical parameters
        try:
            print(f"Processing model with parameters: I_c={I_c}, gamma_c={gamma_c}, beta_c={beta_c}, create_plots={create_plots}")
            results = convert_model_to_physical(
                model_data,
                I_c=I_c,
                gamma_c=gamma_c,
                beta_c=beta_c,
                save_path=output_dir if create_plots else None,
                create_plots=create_plots,
                skip_connections=False  # First try with connections enabled
            )
        except KeyError as ke:
            # Try with connections skipped
            print(f"Standard conversion failed, trying again with connections skipped: {ke}")
            try:
                results = convert_model_to_physical(
                    model_data,
                    I_c=I_c,
                    gamma_c=gamma_c,
                    beta_c=beta_c,
                    save_path=output_dir if create_plots else None,
                    create_plots=create_plots,
                    skip_connections=True  # Skip connections on second attempt
                )
            except Exception as second_attempt_err:
                # If that also fails, try a manual extraction approach
                print(f"Second attempt failed, attempting manual extraction: {second_attempt_err}")
                try:
                    # Extract available parameters manually
                    results = {'base_parameters': {}, 'layers': {}, 'connections': {}}
                    
                    # Set base parameters
                    converter = PhysicalConverter(I_c=I_c, gamma_c=gamma_c, beta_c=beta_c)
                    results['base_parameters'] = converter.get_base_parameters()
                
                    # Extract what we can from the model data
                    if isinstance(model_data, dict):
                        # Try to identify the state dictionary
                        state_dict = None
                        if 'model_state_dict' in model_data:
                            state_dict = model_data['model_state_dict']
                        else:
                            # Use the model data itself if it looks like a state dict
                            state_dict = model_data
                        
                        # Check if state_dict is valid before processing
                        if state_dict and isinstance(state_dict, dict):
                            # Look for layer and connection parameters
                            for key, value in state_dict.items():
                                if not isinstance(value, torch.Tensor):
                                    continue
                                    
                                # Try to extract useful information
                                parts = key.split('.')
                                if 'layers' in key:
                                    # Try to extract layer information
                                    for i, part in enumerate(parts):
                                        if part == 'layers' and i+1 < len(parts):
                                            try:
                                                layer_idx = int(parts[i+1])
                                                if layer_idx not in results['layers']:
                                                    results['layers'][layer_idx] = {}
                                                # Store the parameter
                                                param_name = parts[-1]
                                                results['layers'][layer_idx][param_name] = value.detach().cpu().numpy()
                                            except (ValueError, IndexError):
                                                continue
                except Exception as manual_err:
                    print(f"Manual extraction also failed: {manual_err}")
                    # Instead of failing, provide a minimal results object
                    results = {'base_parameters': {}, 'layers': {}, 'connections': {}}
                    
                    # Set at least the base parameters
                    converter = PhysicalConverter(I_c=I_c, gamma_c=gamma_c, beta_c=beta_c)
                    results['base_parameters'] = converter.get_base_parameters()
        except Exception as extract_err:
            print(f"Error during model conversion: {extract_err}")
            # Instead of failing, provide a minimal results object
            results = {'base_parameters': {}, 'layers': {}, 'connections': {}}
            
            # Set at least the base parameters
            converter = PhysicalConverter(I_c=I_c, gamma_c=gamma_c, beta_c=beta_c)
            results['base_parameters'] = converter.get_base_parameters()
        
        # Also extract parameters dataframe for analysis
        param_df = extract_param_df(model_data)
        
        # If pandas is available, save the dataframe
        try:
            param_df.to_csv(os.path.join(output_dir, 'parameters.csv'))
        except Exception as e:
            print(f"Warning: Could not save parameters dataframe: {e}")
        
        # Collect information about plots
        plots = []
        if create_plots:
            hist_dir = os.path.join(output_dir, 'histograms')
            if os.path.exists(hist_dir):
                for plot_file in os.listdir(hist_dir):
                    if plot_file.endswith('.png'):
                        plots.append({
                            'name': plot_file.replace('.png', '').replace('_', ' '),
                            'path': f'/model_plots/{model_id}/histograms/{plot_file}'
                        })
        
        # Prepare response
        response = {
            'metadata': metadata,
            'results': results,
            'plots': plots,
            'modelId': model_id
        }
        
        return response
    
    def export_model_data(self, 
                         model_id: str, 
                         export_format: str = 'csv',
                         selections: List[str] = None) -> Dict[str, Any]:
        """
        Export model data in the specified format.
        
        Args:
            model_id: ID of the model to export
            export_format: 'csv' or 'json'
            selections: List of data types to export ('summary', 'layers', 'connections', 'plots')
            
        Returns:
            Dictionary with export status and download URL
        """
        if selections is None:
            selections = ['summary', 'layers', 'connections']
            
        # Determine source and destination directories
        source_dir = os.path.join(self.temp_dir, model_id)
        if not os.path.exists(source_dir):
            raise ValueError(f"Model data for ID {model_id} not found")
        
        # Create a unique ID for this export
        export_id = str(uuid.uuid4())
        export_dir = os.path.join(self.temp_dir, f"export_{export_id}")
        os.makedirs(export_dir, exist_ok=True)
        
        # Copy selected data
        if 'summary' in selections:
            if export_format == 'csv':
                summary_csv = os.path.join(source_dir, 'csv', 'parameter_summary.csv')
                if os.path.exists(summary_csv):
                    shutil.copy(summary_csv, export_dir)
            elif export_format == 'json':
                # Load and save summary as JSON
                try:
                    import pandas as pd
                    summary_csv = os.path.join(source_dir, 'csv', 'parameter_summary.csv')
                    if os.path.exists(summary_csv):
                        df = pd.read_csv(summary_csv)
                        with open(os.path.join(export_dir, 'parameter_summary.json'), 'w') as f:
                            f.write(df.to_json(orient='records'))
                except Exception as e:
                    print(f"Warning: Could not export summary as JSON: {e}")
        
        if 'layers' in selections:
            if export_format == 'csv':
                layers_dir = os.path.join(source_dir, 'csv')
                for file in os.listdir(layers_dir):
                    if file.startswith('layer_') and file.endswith('.csv'):
                        shutil.copy(os.path.join(layers_dir, file), export_dir)
            elif export_format == 'json':
                # Load and save layers as JSON
                try:
                    import pandas as pd
                    layers_dir = os.path.join(source_dir, 'csv')
                    for file in os.listdir(layers_dir):
                        if file.startswith('layer_') and file.endswith('.csv'):
                            df = pd.read_csv(os.path.join(layers_dir, file))
                            json_file = file.replace('.csv', '.json')
                            with open(os.path.join(export_dir, json_file), 'w') as f:
                                f.write(df.to_json(orient='records'))
                except Exception as e:
                    print(f"Warning: Could not export layers as JSON: {e}")
        
        if 'connections' in selections:
            if export_format == 'csv':
                conn_dir = os.path.join(source_dir, 'csv')
                for file in os.listdir(conn_dir):
                    if file.startswith('connection_') and file.endswith('.csv'):
                        shutil.copy(os.path.join(conn_dir, file), export_dir)
            elif export_format == 'json':
                # Load and save connections as JSON
                try:
                    import pandas as pd
                    conn_dir = os.path.join(source_dir, 'csv')
                    for file in os.listdir(conn_dir):
                        if file.startswith('connection_') and file.endswith('.csv'):
                            df = pd.read_csv(os.path.join(conn_dir, file))
                            json_file = file.replace('.csv', '.json')
                            with open(os.path.join(export_dir, json_file), 'w') as f:
                                f.write(df.to_json(orient='records'))
                except Exception as e:
                    print(f"Warning: Could not export connections as JSON: {e}")
        
        if 'plots' in selections:
            plots_dir = os.path.join(source_dir, 'histograms')
            if os.path.exists(plots_dir):
                for file in os.listdir(plots_dir):
                    if file.endswith('.png'):
                        shutil.copy(os.path.join(plots_dir, file), export_dir)
        
        # Create a zip file with all exported data
        zip_filename = f"model_data_{export_id}.zip"
        zip_path = os.path.join(self.temp_dir, zip_filename)
        
        shutil.make_archive(
            os.path.join(self.temp_dir, f"model_data_{export_id}"),
            'zip',
            export_dir
        )
        
        # Cleanup temporary export directory
        shutil.rmtree(export_dir)
        
        return {
            'status': 'success',
            'download_url': f'/download/{zip_filename}'
        }
    
    def run_cli(self, args: Optional[List[str]] = None):
        """Run the converter from the command line."""
        parser = argparse.ArgumentParser(description='Convert SOEN model parameters to physical values')
        parser.add_argument('model_path', help='Path to the .pth model file')
        parser.add_argument('--I_c', type=float, default=100e-6, help='Critical current in Amperes (default: 100e-6)')
        parser.add_argument('--gamma_c', type=float, default=1.5e-9, help='Capacitance proportionality in F/A (default: 1.5e-9)')
        parser.add_argument('--beta_c', type=float, default=0.3, help='Stewart-McCumber parameter (default: 0.3)')
        parser.add_argument('--output', '-o', help='Directory to save results')
        parser.add_argument('--no-plots', action='store_true', help='Disable generation of parameter distribution plots')
        
        parsed_args = parser.parse_args(args)
        
        # Convert the model
        result = self.convert_model(
            model_path=parsed_args.model_path,
            I_c=parsed_args.I_c,
            gamma_c=parsed_args.gamma_c,
            beta_c=parsed_args.beta_c,
            create_plots=not parsed_args.no_plots,
            output_dir=parsed_args.output
        )
        
        # Print summary information
        print(f"\nModel Conversion Complete")
        print(f"===========================")
        print(f"Model ID: {result['metadata']['model_id']}")
        print(f"Epochs: {result['metadata'].get('epoch', 'N/A')}")
        print(f"Timestep (dt): {result['metadata'].get('dt', 'N/A')}")
        
        base_params = result['results']['base_parameters']
        print(f"\nBase Physical Parameters:")
        print(f"  Critical Current (I_c): {base_params['I_c']} A")
        print(f"  Capacitance Prop. (Î³_c): {base_params['gamma_c']} F/A")
        print(f"  Stewart-McCumber (Î²_c): {base_params['beta_c']}")
        print(f"  Junction Capacitance (c_j): {base_params['c_j']} F")
        print(f"  Junction Resistance (r_jj): {base_params['r_jj']} Î©")
        
        print(f"\nModel Structure:")
        print(f"  Layer count: {len(result['results']['layers'])}")
        print(f"  Connection count: {len(result['results']['connections'])}")
        
        print(f"\nResults saved to: {parsed_args.output or os.path.join(self.temp_dir, result['metadata']['model_id'])}")
        
        if not parsed_args.no_plots and result['plots']:
            print(f"\nGenerated {len(result['plots'])} parameter distribution plots")

if __name__ == "__main__":
    converter = ModelConverter()
    converter.run_cli()
